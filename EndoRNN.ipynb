{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EndoRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stsan9/EndoMondoResearchERSP/blob/master/EndoRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "17b89a9f-3d04-4bf9-cfd0-a55996b06eb9",
        "id": "xnSW3kcYreIT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import the necessary libraries\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from tensorflow.python.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DATpPO_af7zA",
        "colab_type": "code",
        "outputId": "abdf041f-6824-4f0d-bd75-f2c06a369411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount the google drive file system\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHtqasHEoFDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load in the data file and store it in a list; data in shared drive\n",
        "properPath = '/content/gdrive/My Drive/EndoMondoData/endomondoHR_proper.json' # this may be personalized\n",
        "data = []\n",
        "\n",
        "with open(properPath) as f:\n",
        "    for l in f:\n",
        "        data.append(eval(l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGVEpmf5Pr0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to pandas dataframe and drop the unsused columns\n",
        "dataframe = pd.DataFrame.from_dict(data)\n",
        "dfsave = dataframe\n",
        "dataframe = dataframe.drop(columns = [\"longitude\", \"altitude\", \"latitude\", \"speed\", \"url\", \"id\", \"gender\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij9pg8MKYGV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to extract first element of each list \"l\"\n",
        "def begin(l):\n",
        "    if isinstance(l, list):\n",
        "        return l[0]\n",
        "\n",
        "# function to get the mean of only the middle 300 / 500 timestamps in one workout\n",
        "def mean(l):\n",
        "    return np.mean(l[100:400])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-mFuEgdwGfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get average heart rate and starting timestamp of all workouts\n",
        "dataframe['heart_rate'] = dataframe['heart_rate'].apply(mean)\n",
        "dataframe['timestamp'] = dataframe['timestamp'].apply(begin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KBCxVGTwJKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filtering out suspicious users based on heart rate\n",
        "bad_users = dataframe[dataframe['heart_rate'] > 185] \n",
        "bad_users = dataframe[dataframe['heart_rate'] < 40]\n",
        "dataframe = dataframe[~dataframe.userId.isin(bad_users['userId'].unique())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xz8YOE56_B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encode the sports column\n",
        "one_hot_sport = pd.get_dummies(dataframe.sport)\n",
        "one_hot_sport\n",
        "dataframe = pd.concat([dataframe, one_hot_sport], axis = 1)\n",
        "dataframe = dataframe.drop(columns = \"sport\")\n",
        "#dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__lnLdztrj66",
        "colab_type": "code",
        "outputId": "0d1233be-5066-49c1-8228-fd76e489f4b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_columns = len(dataframe.columns) - 2 # columns - 2 refers to including all the columns except userId and heart_rate\n",
        "num_columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT34wRnSMqzH",
        "colab_type": "code",
        "outputId": "a0309cae-ed81-4fda-e03b-178bf9b4bc85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# number of unique users\n",
        "len(dataframe[\"userId\"].unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1039"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAGbNSe-OK9G",
        "colab_type": "code",
        "outputId": "fb48cdea-6518-4991-f45f-7de83c71c216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# dataframe now only has users who have more than 50 workouts\n",
        "dataframe = dataframe.groupby(\"userId\").filter(lambda x : len(x) > 50)\n",
        "len(dataframe[\"userId\"].unique()) # number of unique users after filtering for those with over 40 workouts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "698"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtrFyzZ3VG4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an object from the Normalizer class\n",
        "min_scaler = MinMaxScaler()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFLj3Y2f6kKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "@params:\n",
        "batch_size: how many testing units we want (how many random users do we want to use in training per function call/steps per epoch)\n",
        "sequence_length: how many workouts per user do we want (e.g. first n number of workouts)\n",
        "\n",
        "@purpose:\n",
        "This function is used to generate the training input for the model. Keras models take in\n",
        "an x and y param, where x is the input, and y is the target output corresponding to the\n",
        "input.\n",
        "\n",
        "@algorithm:\n",
        "Create 2 empty batches. \"x_batch\" represents what the model will directly take in as input.\n",
        "\"y_batch\" represents the true values we want to predict. x and y batches holds data for \n",
        "\"batch_size\" number of users.\n",
        "\n",
        "To fill all indices in the x and y batch, we perform the for loop below and select\n",
        "a user one by one from the \"userids\" list. We extract all of user_x's rows from the dataframe\n",
        "and sort those rows by their timestamp so the input will be properly sequenced for the model.\n",
        "\n",
        "We then drop the columns for userId and timestamp for user_x as those are no longer needed \n",
        "and don't have any correlation to the heart_rate prediction.\n",
        "\n",
        "To turn user_x into what will be input the model can accept, we convert it to a numpy array (keras models \n",
        "don't take pandas dataframes) and then normalize it's values to be between 0 and 1 (neural networks don't \n",
        "work well with large scalars). This final result will be stored in x_scaled, which is then put in x_batch[i].\n",
        "\n",
        "To turn user_data into what will be the input's corresponding target data, we extract only\n",
        "the heart_rate, and convert that into a numpy array. This get's stored in y_out and then\n",
        "y_batch[i].\n",
        "\n",
        "@returns:\n",
        "x_batch: input signals to RNN\n",
        "y_batch: the corresponding target data (heart_rates) to the input signals\n",
        "\"\"\"\n",
        "def batch_gen(batch_size, sequence_length):\n",
        "  userids = dataframe['userId'].unique() # contains all the userids that are good enough to evaluate\n",
        "\n",
        "  while True:\n",
        "  # Allocate a new array for the batch of input_signals.\n",
        "    x_shape = (batch_size, sequence_length, 44) # shape of the input\n",
        "    x_batch = np.zeros(shape=x_shape, dtype=np.float16) # represents the user extracted data used as inputs\n",
        "\n",
        "    # Allocate a new array for the batch of output-signals.\n",
        "    y_shape = (batch_size, sequence_length) #shape of the output\n",
        "    y_batch = np.zeros(shape=y_shape, dtype=np.float16) # represents the user extracted data used as true values\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      user_x = dataframe.loc[dataframe[\"userId\"] == userids[i]].sort_values(\"timestamp\") # get a userId and sort the user by timestamp\n",
        "      user_x = user_x.drop(columns=[\"userId\", \"timestamp\"]) # drop the user's userId and timestamp\n",
        "      y_out = user_x.heart_rate \n",
        "\n",
        "      x_input = user_x.values[0:sequence_length] #inputs used will be from the range (0 - sequence_length)\n",
        "      y_out = y_out.values[10:sequence_length + 10] # trues values (predictions) will go from the range (sequence_length - to the end)\n",
        "        \n",
        "      x_scaled = min_scaler.fit_transform(x_input) # Scale the x_input data from 0 to 1\n",
        "        \n",
        "      x_batch[i] = x_scaled # \n",
        "      y_batch[i] = y_out  \n",
        "    yield (x_batch, y_batch) # returns input signals (x_batch) and corresponding target data(heart_rates) to the input signals (y_batch)\n",
        "\n",
        "generator = batch_gen(40, 40) # (Batch size: 40), (Sequence length: 40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhnFsRH_t56T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Callbacks\n",
        "\n",
        "- Various callbacks allow monitoring of a model to prevent overfitting.\n",
        "- The best version of the model (with specific weights) can be saved\n",
        "- Model will be stopped when loss is not decreasing/is minimized\n",
        "- Using \"EarlyStopping\" and \"ModelCheckpoint\"\n",
        "  - LR Scheduler?\n",
        "\n",
        "Resources\n",
        "- Early Stopping\n",
        "  - https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
        "  - Evaluate + Visualize model based on ^^ link\n",
        "- Model Checkpoint in Google Colab\n",
        "  - https://medium.com/@mukesh.kumar43585/model-checkpoint-google-colab-and-drive-as-persistent-storage-for-long-training-runs-e35ffa0c33d9\n",
        "\"\"\"\n",
        "\n",
        "#Stops the model to minimize loss when there is no more improvement\n",
        "es=EarlyStopping( monitor='val_loss', #quantity to be monitored, validation dataset loss\n",
        "                  mode='min', #minimizing loss\n",
        "                  verbose=1, #print out epoch where training was stopped\n",
        "                  patience=3 #number of epochs with no improvement where training will be stopped\n",
        "                )\n",
        "\n",
        "#Save the best model in training for later use\n",
        "filepath=\"/content/gdrive/My Drive/EndoMondoData/epochs:{epoch:03d}-val_loss:{val_loss:.3f}.hdf5\"\n",
        "mc= ModelCheckpoint( filepath, #path to where model should be saved\n",
        "                     monitor='val_loss', #quanity to be monitored\n",
        "                     mode='min', #minimizing loss\n",
        "                     verbose='1', #print out epoch saved\n",
        "                     save_best_only='True' #saves the best model only, won't overwrite per run\n",
        "                   )\n",
        "\n",
        "#List of callbacks for model\n",
        "callbacks_list=[es, mc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Due3hwgjz2dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load a saved model\n",
        "#saved_model = load_model('best_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t-KWOarbGZK",
        "colab_type": "code",
        "outputId": "65823a3a-f33f-491e-d970-d75895c75619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# baseline for the rnn, the mse error if the predicted values is the mean of previous workouts\n",
        "error_sq = [] # list of error values per user\n",
        "\n",
        "# adds the squared difference of the average per user\n",
        "for user in dataframe['userId'].unique():\n",
        "  user_x = dataframe.loc[dataframe[\"userId\"] == user].sort_values(\"timestamp\", ascending=False)\n",
        "  avg_hr = np.average(user_x.iloc[1: 40]['heart_rate'])\n",
        "  error_sq += [(user_x.iloc[0]['heart_rate'] - avg_hr) ** 2]\n",
        "\n",
        "\n",
        "dummy_mse = np.average(error_sq) # the final MSE value\n",
        "print('Baseline MSE: ' + str(dummy_mse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline MSE: 190.8077249662216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW0AM6mtTrc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model (1 LSTM layer and 1 output layer)\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(units = 256, return_sequences=True, input_shape = (None, num_columns,)))  # \"44\" is the number of columns an input has\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(256, activation = 'tanh'))\n",
        "model.add(Dense(1, activation = 'linear'))  # \"linear\" activation function f(x) = x\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(lr=1e-3) #low learning rate, could change this as well\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)  # using mse loss function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLCoWomYrfmJ",
        "colab_type": "code",
        "outputId": "052ecd7b-1df0-4d02-caf2-b154af33fbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(generator=generator,  # the batch generator\n",
        "          epochs=40,            # number of training cycles\n",
        "          steps_per_epoch=30)#,   # number of calls to generator per cycle\n",
        "          #callbacks=callbacks_list) #list of callbacks to apply - BROKEN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-d249b9747c56>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 30 steps\n",
            "Epoch 1/40\n",
            "30/30 [==============================] - 15s 500ms/step - loss: 15541.7628\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 8s 257ms/step - loss: 12815.3988\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 8s 266ms/step - loss: 11060.9698\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 8s 254ms/step - loss: 9486.6267\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 8s 257ms/step - loss: 8068.3759\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 7s 250ms/step - loss: 6793.0995\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 7s 250ms/step - loss: 5644.5250\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 8s 253ms/step - loss: 4614.8697\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 8s 258ms/step - loss: 3701.1355\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 8s 258ms/step - loss: 2901.9203\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 8s 257ms/step - loss: 2216.2613\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 8s 252ms/step - loss: 1643.1409\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 8s 251ms/step - loss: 1181.2384\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 8s 254ms/step - loss: 828.6211\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 8s 251ms/step - loss: 582.1527\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 8s 254ms/step - loss: 435.9684\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 8s 253ms/step - loss: 376.3522\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 8s 251ms/step - loss: 367.4582\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 8s 255ms/step - loss: 367.3824\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 8s 253ms/step - loss: 367.3990\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 8s 251ms/step - loss: 367.3979\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 8s 252ms/step - loss: 367.3986\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 8s 252ms/step - loss: 367.3989\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 7s 250ms/step - loss: 367.3989\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 8s 250ms/step - loss: 367.3989\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 8s 253ms/step - loss: 367.3989\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 8s 250ms/step - loss: 367.3989\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 8s 252ms/step - loss: 367.3989\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 8s 251ms/step - loss: 367.3989\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 8s 253ms/step - loss: 367.3989\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 8s 251ms/step - loss: 367.3989\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 8s 252ms/step - loss: 367.3989\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 8s 254ms/step - loss: 367.3989\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 8s 254ms/step - loss: 367.3989\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 8s 254ms/step - loss: 367.3989\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 8s 254ms/step - loss: 367.3989\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 8s 251ms/step - loss: 367.3989\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 8s 254ms/step - loss: 367.3989\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 8s 252ms/step - loss: 367.3989\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 8s 251ms/step - loss: 367.3989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdaf3721908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM18jYeylVl8",
        "colab_type": "text"
      },
      "source": [
        "Still need to:\n",
        "- Add callbacks (save the model after training) - SRAVYA \n",
        "- Extract a validation set from the current training set - SRAVYA\n",
        "- Extract a training set and testing set from the current set - ANDRES\n",
        "- Evaluate the model and experiment with adding back in other contextual variables\n",
        "- Modify the data and RNN to output a timestamp as well\n",
        "- Visualize our RNN's predictions"
      ]
    }
  ]
}