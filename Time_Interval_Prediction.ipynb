{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time_Interval_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stsan9/EndoMondoResearchERSP/blob/master/Time_Interval_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xnSW3kcYreIT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20348f40-cd34-4d16-a004-7da8da4ee78c"
      },
      "source": [
        "# Import the necessary libraries\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.keras.layers import Input, Dense, LSTM, Embedding, Dropout, GRU\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from tensorflow.python.keras.models import load_model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DATpPO_af7zA",
        "colab_type": "code",
        "outputId": "8a813f48-1be3-44e1-a37b-14c2ef66c913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Mount the google drive file system\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHtqasHEoFDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load in the data file and store it in a list; data in shared drive\n",
        "properPath = '/content/gdrive/My Drive/endomondoHR_proper.json' # this may be personalized\n",
        "\n",
        "data = []\n",
        "with open(properPath) as f:\n",
        "    for l in f:\n",
        "        data.append(eval(l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGVEpmf5Pr0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to pandas dataframe and drop the unsused columns  \n",
        "dataframe = pd.DataFrame.from_dict(data)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij9pg8MKYGV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to extract first element of each list \"l\"\n",
        "def begin(l):\n",
        "  if isinstance(l, list):\n",
        "      return l[0]\n",
        "\n",
        "# function to get the mean of only the middle 300 / 500 timestamps in one workout\n",
        "def mean(l):\n",
        "  if isinstance(l, list):\n",
        "    return np.mean(l[100:-100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-mFuEgdwGfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get average heart rate and starting timestamp of all workouts\n",
        "dataframe['heart_rate'] = dataframe['heart_rate'].apply(mean)\n",
        "dataframe['timestamp'] = dataframe['timestamp'].apply(begin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Gep7ERSXJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = dataframe.drop(columns = [\"longitude\", \"altitude\", \"latitude\", \"speed\", \"url\", \"id\", \"gender\", \"sport\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KBCxVGTwJKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filtering out suspicious users based on heart rate\n",
        "bad_users = dataframe[dataframe['heart_rate'] > 185]\n",
        "bad_users = dataframe[dataframe['heart_rate'] < 40]\n",
        "dataframe = dataframe[~dataframe.userId.isin(bad_users['userId'].unique())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDG69D4XrMGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_WORKOUTS = 20\n",
        "# dataframe now only has users who have more than specified workouts\n",
        "dataframe = dataframe.groupby(\"userId\").filter(lambda x : len(x) >= NUM_WORKOUTS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wi3DYAI5pwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_users=dataframe[\"userId\"].unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QlPzqeQ65R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for user in all_users:\n",
        "  user_data = dataframe.loc[dataframe[\"userId\"] == user].sort_values(\"timestamp\")\n",
        "  arr = np.diff(user_data[\"timestamp\"])\n",
        "  arr = np.append(arr,[0])\n",
        "  user_data[\"timestamp\"] = arr.tolist()\n",
        "  user_data = user_data[0:NUM_WORKOUTS]\n",
        "  indexNames = dataframe[dataframe[\"userId\"] == user].index\n",
        "  dataframe.drop(indexNames,inplace=True)\n",
        "  dataframe = dataframe.append(user_data,ignore_index = True)\n",
        "\n",
        "dataframe = dataframe.rename( columns= { \"timestamp\" : \"hrs_to_next\" } )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Z2opoRamdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sec_to_hours(secs):\n",
        "  return math.floor((secs / 60 / 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef6idanWbBnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe['hrs_to_next'] = dataframe['hrs_to_next'].apply(sec_to_hours)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzjfHtGG0sUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop users that have > 900 hrs to next workout\n",
        "bad_users = dataframe.loc[dataframe[\"hrs_to_next\"] > 900][\"userId\"].unique()\n",
        "for user in bad_users:\n",
        "  indexNames = dataframe[dataframe[\"userId\"] == user].index\n",
        "  dataframe.drop(indexNames, inplace=True)\n",
        "\n",
        "dataframe = dataframe.reset_index(drop=True)\n",
        "\n",
        "# drop users that have < 5 hrs to next workout\n",
        "bad_users = dataframe.loc[dataframe[\"hrs_to_next\"] < 5][\"userId\"].unique()\n",
        "for user in bad_users:\n",
        "  indexNames = dataframe[dataframe[\"userId\"] == user].index\n",
        "  dataframe.drop(indexNames, inplace=True)\n",
        "\n",
        "dataframe = dataframe.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__lnLdztrj66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# columns - including all the columns except userId\n",
        "num_columns = len(dataframe.columns) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT34wRnSMqzH",
        "colab_type": "code",
        "outputId": "3d0188eb-0866-4a6f-b919-d2790919c0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# number of unique users\n",
        "len(dataframe[\"userId\"].unique())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "305"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtrFyzZ3VG4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an object from the Normalizer class\n",
        "min_scaler = MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yToJhrlVsdDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the users in our training data\n",
        "userids = dataframe['userId'].unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X5mZqCOVJ41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "f4c54b4a-82ae-4942-be06-0b5191f64d96"
      },
      "source": [
        "dataframe"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>heart_rate</th>\n",
              "      <th>hrs_to_next</th>\n",
              "      <th>userId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>152.546667</td>\n",
              "      <td>72</td>\n",
              "      <td>4007546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>155.236667</td>\n",
              "      <td>24</td>\n",
              "      <td>4007546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>147.550000</td>\n",
              "      <td>24</td>\n",
              "      <td>4007546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>153.350000</td>\n",
              "      <td>46</td>\n",
              "      <td>4007546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>157.343333</td>\n",
              "      <td>71</td>\n",
              "      <td>4007546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6095</th>\n",
              "      <td>123.020000</td>\n",
              "      <td>62</td>\n",
              "      <td>1875839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6096</th>\n",
              "      <td>134.740000</td>\n",
              "      <td>97</td>\n",
              "      <td>1875839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6097</th>\n",
              "      <td>121.393333</td>\n",
              "      <td>95</td>\n",
              "      <td>1875839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6098</th>\n",
              "      <td>124.550000</td>\n",
              "      <td>50</td>\n",
              "      <td>1875839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6099</th>\n",
              "      <td>128.086667</td>\n",
              "      <td>29</td>\n",
              "      <td>1875839</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      heart_rate  hrs_to_next   userId\n",
              "0     152.546667           72  4007546\n",
              "1     155.236667           24  4007546\n",
              "2     147.550000           24  4007546\n",
              "3     153.350000           46  4007546\n",
              "4     157.343333           71  4007546\n",
              "...          ...          ...      ...\n",
              "6095  123.020000           62  1875839\n",
              "6096  134.740000           97  1875839\n",
              "6097  121.393333           95  1875839\n",
              "6098  124.550000           50  1875839\n",
              "6099  128.086667           29  1875839\n",
              "\n",
              "[6100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HQKc_Rd4gIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rolling_window(a, window_size):\n",
        "    shape = (a.shape[0] - window_size + 1, window_size) + a.shape[1:]\n",
        "    strides = (a.strides[0],) + a.strides\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t-KWOarbGZK",
        "colab_type": "code",
        "outputId": "9fafe3de-8e19-43c0-ca2d-e50a14b01fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# baseline for the rnn, the mse error if the predicted values is the mean of previous workouts\n",
        "error_sq = [] # list of error values per user\n",
        "\n",
        "# adds the squared difference of the average per user\n",
        "for user in dataframe['userId'].unique():\n",
        "  user_x = dataframe.loc[dataframe[\"userId\"] == user]\n",
        "\n",
        "  for i in range(11): # implement sliding window\n",
        "    avg_hr = np.average(user_x.iloc[0+i: 9+i]['hrs_to_next'])\n",
        "    error_sq += [(np.log(user_x.iloc[9+i]['hrs_to_next']) - np.log(avg_hr)) ** 2]\n",
        "\n",
        "dummy_mse = np.average(error_sq) # the final MSE value\n",
        "print('Baseline MSE for hours: ' + str(dummy_mse))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline MSE for hours: 0.7654247881055768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW0AM6mtTrc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model (1 LSTM layer and 1 output layer)\n",
        "model = Sequential()\n",
        "\n",
        "model.add(GRU(units = 32, return_sequences=False, input_shape = (None, num_columns,)))\n",
        "model.add(Dropout(0.1))\n",
        "#model.add(Dense(8, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'relu'))\n",
        "\n",
        "#optimizer = tf.keras.optimizers.RMSprop(lr=1e-3) #low learning rate, could change this as well\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')  # using mse loss function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLCoWomYrfmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42e395bf-0fb1-42ec-a9b2-ee5ad0ebaffe"
      },
      "source": [
        "epochs = 200\n",
        "batch_size = 10\n",
        "num_users = len(userids)\n",
        "window_size = 10\n",
        "sequence_length = 20\n",
        "\n",
        "for e in range(epochs):\n",
        "  np.random.shuffle(userids)\n",
        "\n",
        "  # initialize batches\n",
        "  x_shape = (num_users*10, window_size, num_columns)\n",
        "  y_shape = (num_users*10, 1)\n",
        "  x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
        "  y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
        "\n",
        "  for b in range(num_users):\n",
        "    # grab 20 workouts from the user\n",
        "    x = dataframe[b * sequence_length : (b + 1) * sequence_length - 1]\n",
        "    x = x.drop(columns=['heart_rate', \"userId\"])\n",
        "    x = x.values\n",
        "\n",
        "    # creates sliding windows for those workouts\n",
        "    sliding_window_x = rolling_window(x, window_size)\n",
        "\n",
        "    for i in range(sliding_window_x.shape[0]):  # put each window into batch\n",
        "      y = dataframe.loc[[(b+1)*window_size+i]]\n",
        "      y = y.hrs_to_next\n",
        "      y = np.log(y, where = y > 0)\n",
        "      x_batch[sliding_window_x.shape[0]*b+i] = sliding_window_x[i]\n",
        "      y_batch[sliding_window_x.shape[0]*b+i] = y\n",
        "\n",
        "  model.fit(x_batch, y_batch, batch_size=batch_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 3s 904us/sample - loss: 0.9648\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.9816\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 791us/sample - loss: 0.9478\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.9453\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 762us/sample - loss: 0.9022\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.8934\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 759us/sample - loss: 0.9138\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 745us/sample - loss: 0.8898\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 751us/sample - loss: 0.8978\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.8891\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 745us/sample - loss: 0.8802\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 752us/sample - loss: 0.8802\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 752us/sample - loss: 0.8777\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 802us/sample - loss: 0.8863\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 746us/sample - loss: 0.8611\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 756us/sample - loss: 0.8597\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 754us/sample - loss: 0.8683\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 752us/sample - loss: 0.8706\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 740us/sample - loss: 0.8694\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 738us/sample - loss: 0.8628\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 732us/sample - loss: 0.8754\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 720us/sample - loss: 0.8580\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 716us/sample - loss: 0.8562\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 737us/sample - loss: 0.8488\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 758us/sample - loss: 0.8568\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 737us/sample - loss: 0.8709\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.8802\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 745us/sample - loss: 0.8588\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 748us/sample - loss: 0.8542\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 739us/sample - loss: 0.8444\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.8481\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.8522\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8613\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 747us/sample - loss: 0.8550\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 738us/sample - loss: 0.8554\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 762us/sample - loss: 0.8607\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.8579\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 759us/sample - loss: 0.8597\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 730us/sample - loss: 0.8404\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 800us/sample - loss: 0.8467\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 793us/sample - loss: 0.8453\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.8441\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 733us/sample - loss: 0.8478\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 741us/sample - loss: 0.8491\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 743us/sample - loss: 0.8411\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 748us/sample - loss: 0.8382\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.8414\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 747us/sample - loss: 0.8603\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 738us/sample - loss: 0.8512\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 740us/sample - loss: 0.8512\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 733us/sample - loss: 0.8372\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8354\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 724us/sample - loss: 0.8549\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 730us/sample - loss: 0.8413\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 722us/sample - loss: 0.8389\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 3s 843us/sample - loss: 0.8413\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 760us/sample - loss: 0.8445\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 741us/sample - loss: 0.8489\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 737us/sample - loss: 0.8375\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 740us/sample - loss: 0.8363\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 731us/sample - loss: 0.8464\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8482\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 741us/sample - loss: 0.8579\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 748us/sample - loss: 0.8350\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 729us/sample - loss: 0.8325\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 731us/sample - loss: 0.8324\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 735us/sample - loss: 0.8407\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 772us/sample - loss: 0.8312\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 752us/sample - loss: 0.8356\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 760us/sample - loss: 0.8360\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 761us/sample - loss: 0.8373\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 755us/sample - loss: 0.8392\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 741us/sample - loss: 0.8294\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 748us/sample - loss: 0.8466\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 763us/sample - loss: 0.8343\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 736us/sample - loss: 0.8360\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 731us/sample - loss: 0.8436\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.8311\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 752us/sample - loss: 0.8292\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.8264\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 745us/sample - loss: 0.8333\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 768us/sample - loss: 0.8405\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8368\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.8358\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.8312\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 736us/sample - loss: 0.8231\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.8261\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 755us/sample - loss: 0.8219\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 767us/sample - loss: 0.8391\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 772us/sample - loss: 0.8304\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 3s 952us/sample - loss: 0.8243\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 736us/sample - loss: 0.8304\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 756us/sample - loss: 0.8278\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 743us/sample - loss: 0.8361\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8210\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 3s 822us/sample - loss: 0.8356\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 3s 821us/sample - loss: 0.8185\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 728us/sample - loss: 0.8193\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 752us/sample - loss: 0.8294\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 815us/sample - loss: 0.8312\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.8179\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 765us/sample - loss: 0.8263\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 765us/sample - loss: 0.8279\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 761us/sample - loss: 0.8308\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 752us/sample - loss: 0.8214\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.8265\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 754us/sample - loss: 0.8183\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 752us/sample - loss: 0.8302\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 729us/sample - loss: 0.8300\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 3s 856us/sample - loss: 0.8251\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 739us/sample - loss: 0.8269\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 762us/sample - loss: 0.8188\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 716us/sample - loss: 0.8225\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 721us/sample - loss: 0.8099\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8244\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 727us/sample - loss: 0.8185\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 729us/sample - loss: 0.8236\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 729us/sample - loss: 0.8280\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 735us/sample - loss: 0.8310\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 760us/sample - loss: 0.8172\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 770us/sample - loss: 0.8129\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 730us/sample - loss: 0.8135\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 749us/sample - loss: 0.8192\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 731us/sample - loss: 0.8266\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 730us/sample - loss: 0.8111\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 729us/sample - loss: 0.8215\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 738us/sample - loss: 0.8239\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 748us/sample - loss: 0.8121\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 721us/sample - loss: 0.8239\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.8181\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 727us/sample - loss: 0.8166\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 735us/sample - loss: 0.8138\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 731us/sample - loss: 0.8141\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.8159\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.8109\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 738us/sample - loss: 0.8214\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 736us/sample - loss: 0.8185\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 736us/sample - loss: 0.8136\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.8084\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 727us/sample - loss: 0.8134\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 731us/sample - loss: 0.8170\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8076\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 770us/sample - loss: 0.8040\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 745us/sample - loss: 0.8103\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 726us/sample - loss: 0.8130\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 751us/sample - loss: 0.8174\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 738us/sample - loss: 0.8169\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 724us/sample - loss: 0.8071\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 713us/sample - loss: 0.8067\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 723us/sample - loss: 0.8096\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.8034\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 812us/sample - loss: 0.8183\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 769us/sample - loss: 0.8082\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 739us/sample - loss: 0.8167\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 737us/sample - loss: 0.8098\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 713us/sample - loss: 0.8102\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 736us/sample - loss: 0.8139\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 721us/sample - loss: 0.8131\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 732us/sample - loss: 0.8161\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.8068\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 718us/sample - loss: 0.8049\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 725us/sample - loss: 0.8086\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8102\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 751us/sample - loss: 0.8064\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 754us/sample - loss: 0.8040\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 3s 850us/sample - loss: 0.8015\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 716us/sample - loss: 0.8075\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 731us/sample - loss: 0.8058\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 691us/sample - loss: 0.7929\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.8069\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 716us/sample - loss: 0.8064\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 726us/sample - loss: 0.8057\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 709us/sample - loss: 0.8083\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 749us/sample - loss: 0.8061\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 763us/sample - loss: 0.8054\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 732us/sample - loss: 0.7916\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 753us/sample - loss: 0.8121\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 764us/sample - loss: 0.7962\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 733us/sample - loss: 0.8031\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 711us/sample - loss: 0.8135\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 754us/sample - loss: 0.7964\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 761us/sample - loss: 0.7902\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 742us/sample - loss: 0.7975\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 722us/sample - loss: 0.8021\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 741us/sample - loss: 0.7991\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 768us/sample - loss: 0.7971\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 736us/sample - loss: 0.7999\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 746us/sample - loss: 0.8018\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 749us/sample - loss: 0.7982\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 759us/sample - loss: 0.7950\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 717us/sample - loss: 0.7937\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 730us/sample - loss: 0.7947\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 756us/sample - loss: 0.7880\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 729us/sample - loss: 0.7993\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 728us/sample - loss: 0.7936\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 744us/sample - loss: 0.8021\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 759us/sample - loss: 0.7979\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 735us/sample - loss: 0.7995\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 734us/sample - loss: 0.7920\n",
            "Train on 3050 samples\n",
            "3050/3050 [==============================] - 2s 729us/sample - loss: 0.7945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh_iCR5Iz9T3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a918708c-9868-45a3-8292-1ceb28081a83"
      },
      "source": [
        "# predictions vs actual\n",
        "for b in range(num_users):\n",
        "  for i in range(11):\n",
        "    x = dataframe[b * window_size + i : (b + 1) * window_size - 1 + i]\n",
        "    x = x.drop(columns=['heart_rate', \"userId\"])\n",
        "    x = x.values\n",
        "    test_shape = (1, window_size - 1, num_columns)\n",
        "    test_input = np.zeros(shape=test_shape, dtype=np.float16)\n",
        "    test_input[0] = x\n",
        "    p = model.predict(test_input) # the prediction\n",
        "    print (\"pred : \" + str(np.exp(p[0][0])) + \" | y: \" + str(dataframe.loc[[(b + 1) * window_size - 1 + i]][\"hrs_to_next\"].iloc(0)[0]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred : 58.804913 | y: 26\n",
            "pred : 64.00369 | y: 29\n",
            "pred : 65.23304 | y: 39\n",
            "pred : 60.68793 | y: 97\n",
            "pred : 60.21818 | y: 25\n",
            "pred : 64.20127 | y: 45\n",
            "pred : 63.609837 | y: 72\n",
            "pred : 56.720005 | y: 24\n",
            "pred : 63.726234 | y: 70\n",
            "pred : 61.45412 | y: 72\n",
            "pred : 61.872528 | y: 26\n",
            "pred : 61.872528 | y: 26\n",
            "pred : 62.977345 | y: 56\n",
            "pred : 61.988525 | y: 111\n",
            "pred : 61.612682 | y: 168\n",
            "pred : 57.593 | y: 295\n",
            "pred : 60.515156 | y: 65\n",
            "pred : 59.72526 | y: 144\n",
            "pred : 56.724304 | y: 312\n",
            "pred : 59.65837 | y: 337\n",
            "pred : 58.328403 | y: 167\n",
            "pred : 50.83742 | y: 117\n",
            "pred : 50.83742 | y: 117\n",
            "pred : 50.762753 | y: 120\n",
            "pred : 50.913597 | y: 24\n",
            "pred : 58.29437 | y: 74\n",
            "pred : 54.683403 | y: 817\n",
            "pred : 58.090363 | y: 43\n",
            "pred : 63.789288 | y: 339\n",
            "pred : 58.1342 | y: 314\n",
            "pred : 58.134117 | y: 186\n",
            "pred : 58.29056 | y: 169\n",
            "pred : 59.679768 | y: 197\n",
            "pred : 59.679768 | y: 197\n",
            "pred : 58.102386 | y: 167\n",
            "pred : 57.97779 | y: 27\n",
            "pred : 61.424503 | y: 118\n",
            "pred : 58.175438 | y: 357\n",
            "pred : 58.181152 | y: 335\n",
            "pred : 58.181152 | y: 167\n",
            "pred : 58.169277 | y: 335\n",
            "pred : 58.189556 | y: 504\n",
            "pred : 58.175686 | y: 325\n",
            "pred : 58.977108 | y: 23\n",
            "pred : 58.977108 | y: 23\n",
            "pred : 53.580208 | y: 19\n",
            "pred : 53.805485 | y: 168\n",
            "pred : 59.10532 | y: 96\n",
            "pred : 59.162586 | y: 75\n",
            "pred : 59.75631 | y: 18\n",
            "pred : 63.779648 | y: 55\n",
            "pred : 63.23788 | y: 65\n",
            "pred : 63.275826 | y: 21\n",
            "pred : 64.61438 | y: 26\n",
            "pred : 66.331314 | y: 121\n",
            "pred : 66.331314 | y: 121\n",
            "pred : 57.31326 | y: 23\n",
            "pred : 62.326588 | y: 24\n",
            "pred : 63.06443 | y: 23\n",
            "pred : 67.3454 | y: 25\n",
            "pred : 64.16721 | y: 70\n",
            "pred : 60.888107 | y: 24\n",
            "pred : 67.162926 | y: 23\n",
            "pred : 60.99112 | y: 24\n",
            "pred : 60.775417 | y: 50\n",
            "pred : 57.34289 | y: 45\n",
            "pred : 57.34289 | y: 45\n",
            "pred : 59.522633 | y: 23\n",
            "pred : 64.50673 | y: 24\n",
            "pred : 62.851734 | y: 23\n",
            "pred : 58.313858 | y: 50\n",
            "pred : 54.70296 | y: 69\n",
            "pred : 59.500698 | y: 23\n",
            "pred : 66.42228 | y: 74\n",
            "pred : 60.10157 | y: 69\n",
            "pred : 61.101006 | y: 24\n",
            "pred : 66.12535 | y: 50\n",
            "pred : 66.12535 | y: 50\n",
            "pred : 64.403305 | y: 77\n",
            "pred : 61.569275 | y: 42\n",
            "pred : 63.762894 | y: 24\n",
            "pred : 67.50078 | y: 192\n",
            "pred : 60.098705 | y: 75\n",
            "pred : 58.06137 | y: 42\n",
            "pred : 66.59077 | y: 53\n",
            "pred : 63.884686 | y: 284\n",
            "pred : 56.919197 | y: 47\n",
            "pred : 65.11414 | y: 47\n",
            "pred : 65.11414 | y: 47\n",
            "pred : 64.705025 | y: 48\n",
            "pred : 62.284252 | y: 24\n",
            "pred : 63.23378 | y: 48\n",
            "pred : 64.122894 | y: 28\n",
            "pred : 65.071434 | y: 19\n",
            "pred : 64.891685 | y: 95\n",
            "pred : 60.582455 | y: 28\n",
            "pred : 64.65577 | y: 18\n",
            "pred : 63.703934 | y: 23\n",
            "pred : 65.370674 | y: 102\n",
            "pred : 65.370674 | y: 102\n",
            "pred : 62.76159 | y: 46\n",
            "pred : 68.19701 | y: 122\n",
            "pred : 59.974087 | y: 69\n",
            "pred : 62.72904 | y: 146\n",
            "pred : 61.705883 | y: 48\n",
            "pred : 60.30582 | y: 240\n",
            "pred : 61.253983 | y: 43\n",
            "pred : 64.54017 | y: 166\n",
            "pred : 58.230194 | y: 125\n",
            "pred : 55.338783 | y: 42\n",
            "pred : 55.338783 | y: 42\n",
            "pred : 62.083157 | y: 168\n",
            "pred : 56.916996 | y: 340\n",
            "pred : 58.078094 | y: 77\n",
            "pred : 57.51972 | y: 91\n",
            "pred : 58.48707 | y: 410\n",
            "pred : 57.22822 | y: 287\n",
            "pred : 57.222626 | y: 93\n",
            "pred : 58.542427 | y: 241\n",
            "pred : 48.792194 | y: 48\n",
            "pred : 58.334522 | y: 193\n",
            "pred : 58.334522 | y: 193\n",
            "pred : 55.28258 | y: 168\n",
            "pred : 54.017826 | y: 166\n",
            "pred : 54.87042 | y: 47\n",
            "pred : 60.295357 | y: 72\n",
            "pred : 56.358067 | y: 145\n",
            "pred : 57.16372 | y: 22\n",
            "pred : 62.042732 | y: 47\n",
            "pred : 60.59606 | y: 47\n",
            "pred : 61.65224 | y: 48\n",
            "pred : 61.593147 | y: 95\n",
            "pred : 61.593147 | y: 95\n",
            "pred : 59.84137 | y: 24\n",
            "pred : 61.575325 | y: 71\n",
            "pred : 58.514015 | y: 48\n",
            "pred : 65.57363 | y: 119\n",
            "pred : 61.32114 | y: 47\n",
            "pred : 64.55402 | y: 336\n",
            "pred : 59.07173 | y: 120\n",
            "pred : 57.76649 | y: 95\n",
            "pred : 59.799408 | y: 239\n",
            "pred : 58.546894 | y: 48\n",
            "pred : 58.546894 | y: 48\n",
            "pred : 62.96374 | y: 44\n",
            "pred : 62.104652 | y: 49\n",
            "pred : 62.49628 | y: 51\n",
            "pred : 59.12133 | y: 116\n",
            "pred : 55.756405 | y: 51\n",
            "pred : 59.802376 | y: 259\n",
            "pred : 57.08342 | y: 24\n",
            "pred : 61.796696 | y: 51\n",
            "pred : 60.5267 | y: 216\n",
            "pred : 59.06368 | y: 68\n",
            "pred : 59.06368 | y: 68\n",
            "pred : 59.360706 | y: 119\n",
            "pred : 57.593987 | y: 48\n",
            "pred : 64.01959 | y: 51\n",
            "pred : 62.620674 | y: 23\n",
            "pred : 67.43476 | y: 24\n",
            "pred : 68.27067 | y: 119\n",
            "pred : 57.506447 | y: 405\n",
            "pred : 58.532852 | y: 165\n",
            "pred : 58.613876 | y: 171\n",
            "pred : 59.996742 | y: 44\n",
            "pred : 59.996742 | y: 44\n",
            "pred : 65.23335 | y: 54\n",
            "pred : 63.196705 | y: 167\n",
            "pred : 61.157963 | y: 118\n",
            "pred : 59.27483 | y: 218\n",
            "pred : 57.36387 | y: 502\n",
            "pred : 57.354954 | y: 44\n",
            "pred : 62.78949 | y: 167\n",
            "pred : 58.373924 | y: 96\n",
            "pred : 57.51119 | y: 29\n",
            "pred : 60.476154 | y: 54\n",
            "pred : 60.476154 | y: 54\n",
            "pred : 57.90571 | y: 304\n",
            "pred : 57.444042 | y: 68\n",
            "pred : 58.811 | y: 312\n",
            "pred : 58.4265 | y: 839\n",
            "pred : 57.336494 | y: 170\n",
            "pred : 57.57971 | y: 95\n",
            "pred : 59.519398 | y: 139\n",
            "pred : 58.202156 | y: 167\n",
            "pred : 50.494915 | y: 99\n",
            "pred : 51.90118 | y: 68\n",
            "pred : 51.90118 | y: 68\n",
            "pred : 50.651867 | y: 48\n",
            "pred : 58.353466 | y: 23\n",
            "pred : 62.113033 | y: 22\n",
            "pred : 63.126682 | y: 49\n",
            "pred : 60.56345 | y: 23\n",
            "pred : 63.766113 | y: 144\n",
            "pred : 57.571583 | y: 23\n",
            "pred : 63.816395 | y: 49\n",
            "pred : 63.527084 | y: 58\n",
            "pred : 62.944893 | y: 23\n",
            "pred : 62.944893 | y: 23\n",
            "pred : 68.21535 | y: 47\n",
            "pred : 66.125755 | y: 23\n",
            "pred : 67.11593 | y: 48\n",
            "pred : 61.005314 | y: 23\n",
            "pred : 63.77019 | y: 23\n",
            "pred : 67.6852 | y: 60\n",
            "pred : 59.143604 | y: 23\n",
            "pred : 59.95622 | y: 37\n",
            "pred : 66.159004 | y: 167\n",
            "pred : 61.71848 | y: 59\n",
            "pred : 61.71848 | y: 59\n",
            "pred : 64.67378 | y: 71\n",
            "pred : 61.350826 | y: 47\n",
            "pred : 67.812454 | y: 19\n",
            "pred : 67.31416 | y: 71\n",
            "pred : 62.026134 | y: 23\n",
            "pred : 67.4129 | y: 24\n",
            "pred : 64.645935 | y: 55\n",
            "pred : 61.66797 | y: 15\n",
            "pred : 60.62063 | y: 48\n",
            "pred : 61.995564 | y: 6\n",
            "pred : 61.995564 | y: 6\n",
            "pred : 67.43457 | y: 23\n",
            "pred : 86.89431 | y: 17\n",
            "pred : 91.40492 | y: 6\n",
            "pred : 95.91682 | y: 25\n",
            "pred : 130.3899 | y: 40\n",
            "pred : 92.15649 | y: 128\n",
            "pred : 137.02522 | y: 142\n",
            "pred : 86.03846 | y: 39\n",
            "pred : 62.04335 | y: 56\n",
            "pred : 35.59244 | y: 15\n",
            "pred : 35.59244 | y: 15\n",
            "pred : 69.521645 | y: 480\n",
            "pred : 55.28975 | y: 169\n",
            "pred : 63.14992 | y: 167\n",
            "pred : 58.29737 | y: 90\n",
            "pred : 59.22624 | y: 244\n",
            "pred : 61.819565 | y: 281\n",
            "pred : 53.953804 | y: 49\n",
            "pred : 45.333492 | y: 264\n",
            "pred : 58.81501 | y: 413\n",
            "pred : 54.291393 | y: 238\n",
            "pred : 54.291393 | y: 238\n",
            "pred : 54.298176 | y: 97\n",
            "pred : 54.91866 | y: 70\n",
            "pred : 56.917976 | y: 96\n",
            "pred : 56.759422 | y: 66\n",
            "pred : 58.482048 | y: 172\n",
            "pred : 52.62885 | y: 43\n",
            "pred : 60.250404 | y: 245\n",
            "pred : 55.054897 | y: 166\n",
            "pred : 55.05726 | y: 144\n",
            "pred : 56.468224 | y: 71\n",
            "pred : 56.468224 | y: 71\n",
            "pred : 55.222824 | y: 25\n",
            "pred : 60.832214 | y: 37\n",
            "pred : 61.850937 | y: 18\n",
            "pred : 61.50261 | y: 252\n",
            "pred : 55.988884 | y: 167\n",
            "pred : 55.97949 | y: 34\n",
            "pred : 63.103268 | y: 38\n",
            "pred : 63.946342 | y: 94\n",
            "pred : 61.373005 | y: 25\n",
            "pred : 66.51048 | y: 81\n",
            "pred : 66.51048 | y: 81\n",
            "pred : 56.055187 | y: 40\n",
            "pred : 66.84238 | y: 46\n",
            "pred : 63.119217 | y: 151\n",
            "pred : 60.115013 | y: 18\n",
            "pred : 63.14914 | y: 24\n",
            "pred : 65.29435 | y: 173\n",
            "pred : 62.54341 | y: 21\n",
            "pred : 65.1329 | y: 27\n",
            "pred : 67.01487 | y: 120\n",
            "pred : 61.21331 | y: 24\n",
            "pred : 61.21331 | y: 24\n",
            "pred : 61.097275 | y: 23\n",
            "pred : 63.271694 | y: 127\n",
            "pred : 64.2345 | y: 283\n",
            "pred : 60.729992 | y: 76\n",
            "pred : 61.30547 | y: 43\n",
            "pred : 69.261345 | y: 47\n",
            "pred : 65.39905 | y: 51\n",
            "pred : 64.71311 | y: 17\n",
            "pred : 66.99264 | y: 96\n",
            "pred : 57.953026 | y: 48\n",
            "pred : 57.953026 | y: 48\n",
            "pred : 61.285244 | y: 93\n",
            "pred : 58.020435 | y: 94\n",
            "pred : 62.23028 | y: 102\n",
            "pred : 60.45038 | y: 234\n",
            "pred : 59.69881 | y: 100\n",
            "pred : 50.377815 | y: 192\n",
            "pred : 59.299793 | y: 17\n",
            "pred : 57.855366 | y: 101\n",
            "pred : 50.916473 | y: 48\n",
            "pred : 56.130558 | y: 68\n",
            "pred : 56.130558 | y: 68\n",
            "pred : 52.474815 | y: 272\n",
            "pred : 51.515446 | y: 167\n",
            "pred : 51.379906 | y: 65\n",
            "pred : 51.87379 | y: 332\n",
            "pred : 52.748264 | y: 105\n",
            "pred : 59.441856 | y: 63\n",
            "pred : 58.996742 | y: 505\n",
            "pred : 55.87841 | y: 167\n",
            "pred : 54.24657 | y: 506\n",
            "pred : 54.219666 | y: 54\n",
            "pred : 54.219666 | y: 54\n",
            "pred : 59.49125 | y: 167\n",
            "pred : 54.73701 | y: 47\n",
            "pred : 60.141216 | y: 64\n",
            "pred : 58.12999 | y: 103\n",
            "pred : 56.68069 | y: 119\n",
            "pred : 56.678585 | y: 48\n",
            "pred : 62.515327 | y: 64\n",
            "pred : 58.798492 | y: 55\n",
            "pred : 59.747707 | y: 48\n",
            "pred : 63.061214 | y: 119\n",
            "pred : 63.061214 | y: 119\n",
            "pred : 56.8731 | y: 427\n",
            "pred : 56.51375 | y: 164\n",
            "pred : 56.81206 | y: 29\n",
            "pred : 61.22761 | y: 218\n",
            "pred : 56.661236 | y: 384\n",
            "pred : 57.34817 | y: 123\n",
            "pred : 57.873272 | y: 255\n",
            "pred : 57.790325 | y: 79\n",
            "pred : 58.278164 | y: 255\n",
            "pred : 58.231247 | y: 408\n",
            "pred : 58.231247 | y: 408\n",
            "pred : 59.23505 | y: 632\n",
            "pred : 46.817394 | y: 162\n",
            "pred : 44.2144 | y: 45\n",
            "pred : 58.61642 | y: 551\n",
            "pred : 54.56507 | y: 48\n",
            "pred : 60.204113 | y: 71\n",
            "pred : 57.642563 | y: 551\n",
            "pred : 57.294926 | y: 165\n",
            "pred : 57.285965 | y: 216\n",
            "pred : 57.285965 | y: 407\n",
            "pred : 57.285965 | y: 407\n",
            "pred : 58.37022 | y: 26\n",
            "pred : 61.220196 | y: 46\n",
            "pred : 60.554066 | y: 48\n",
            "pred : 56.778614 | y: 144\n",
            "pred : 58.51558 | y: 46\n",
            "pred : 63.93485 | y: 21\n",
            "pred : 65.99563 | y: 52\n",
            "pred : 64.52069 | y: 46\n",
            "pred : 67.26661 | y: 24\n",
            "pred : 68.655396 | y: 45\n",
            "pred : 68.655396 | y: 45\n",
            "pred : 65.37644 | y: 26\n",
            "pred : 65.418755 | y: 70\n",
            "pred : 60.83088 | y: 25\n",
            "pred : 61.69997 | y: 22\n",
            "pred : 66.427536 | y: 23\n",
            "pred : 64.31118 | y: 47\n",
            "pred : 58.82853 | y: 96\n",
            "pred : 62.394714 | y: 23\n",
            "pred : 65.48792 | y: 119\n",
            "pred : 60.88555 | y: 24\n",
            "pred : 60.88555 | y: 24\n",
            "pred : 65.94196 | y: 18\n",
            "pred : 65.85916 | y: 43\n",
            "pred : 66.357666 | y: 36\n",
            "pred : 66.80194 | y: 113\n",
            "pred : 60.351963 | y: 98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7ff229e399d8>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/weakref.py\", line 356, in remove\n",
            "    def remove(k, selfref=ref(self)):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred : 62.429295 | y: 21\n",
            "pred : 66.27295 | y: 25\n",
            "pred : 67.43698 | y: 46\n",
            "pred : 59.877647 | y: 30\n",
            "pred : 66.544174 | y: 44\n",
            "pred : 66.544174 | y: 44\n",
            "pred : 63.787098 | y: 27\n",
            "pred : 56.292667 | y: 63\n",
            "pred : 52.328316 | y: 23\n",
            "pred : 56.590843 | y: 77\n",
            "pred : 58.193607 | y: 26\n",
            "pred : 64.002014 | y: 40\n",
            "pred : 62.067528 | y: 46\n",
            "pred : 62.791164 | y: 24\n",
            "pred : 63.40688 | y: 45\n",
            "pred : 62.174877 | y: 51\n",
            "pred : 62.174877 | y: 51\n",
            "pred : 60.87246 | y: 135\n",
            "pred : 59.626976 | y: 192\n",
            "pred : 60.232883 | y: 95\n",
            "pred : 61.157757 | y: 550\n",
            "pred : 59.234993 | y: 337\n",
            "pred : 59.59545 | y: 191\n",
            "pred : 59.99188 | y: 227\n",
            "pred : 56.277718 | y: 115\n",
            "pred : 39.1001 | y: 52\n",
            "pred : 55.373283 | y: 48\n",
            "pred : 55.373283 | y: 48\n",
            "pred : 60.002808 | y: 335\n",
            "pred : 56.426636 | y: 227\n",
            "pred : 56.426636 | y: 108\n",
            "pred : 56.46849 | y: 120\n",
            "pred : 56.48896 | y: 47\n",
            "pred : 61.24236 | y: 119\n",
            "pred : 57.84858 | y: 167\n",
            "pred : 58.021767 | y: 167\n",
            "pred : 56.27737 | y: 47\n",
            "pred : 61.364838 | y: 119\n",
            "pred : 61.364838 | y: 119\n",
            "pred : 56.685097 | y: 22\n",
            "pred : 60.930832 | y: 47\n",
            "pred : 61.84681 | y: 48\n",
            "pred : 61.09381 | y: 311\n",
            "pred : 58.66681 | y: 71\n",
            "pred : 58.927692 | y: 25\n",
            "pred : 64.18169 | y: 24\n",
            "pred : 63.28337 | y: 118\n",
            "pred : 61.233307 | y: 25\n",
            "pred : 65.36896 | y: 47\n",
            "pred : 65.36896 | y: 47\n",
            "pred : 62.3002 | y: 47\n",
            "pred : 60.288025 | y: 215\n",
            "pred : 58.18179 | y: 335\n",
            "pred : 61.103104 | y: 48\n",
            "pred : 67.43293 | y: 361\n",
            "pred : 60.105667 | y: 73\n",
            "pred : 59.90001 | y: 22\n",
            "pred : 63.663517 | y: 119\n",
            "pred : 59.2802 | y: 120\n",
            "pred : 58.27005 | y: 47\n",
            "pred : 58.27005 | y: 47\n",
            "pred : 64.1154 | y: 48\n",
            "pred : 64.38445 | y: 98\n",
            "pred : 56.930244 | y: 21\n",
            "pred : 61.82888 | y: 56\n",
            "pred : 62.238087 | y: 96\n",
            "pred : 60.858818 | y: 14\n",
            "pred : 55.057182 | y: 69\n",
            "pred : 57.97005 | y: 120\n",
            "pred : 57.466698 | y: 24\n",
            "pred : 64.491936 | y: 49\n",
            "pred : 64.491936 | y: 49\n",
            "pred : 68.73378 | y: 92\n",
            "pred : 66.25722 | y: 28\n",
            "pred : 60.057854 | y: 99\n",
            "pred : 55.31496 | y: 42\n",
            "pred : 66.36577 | y: 129\n",
            "pred : 58.637417 | y: 118\n",
            "pred : 60.09595 | y: 39\n",
            "pred : 65.46735 | y: 119\n",
            "pred : 57.43544 | y: 203\n",
            "pred : 59.597584 | y: 39\n",
            "pred : 59.597584 | y: 39\n",
            "pred : 63.975224 | y: 67\n",
            "pred : 59.14132 | y: 144\n",
            "pred : 57.35763 | y: 100\n",
            "pred : 57.451878 | y: 144\n",
            "pred : 58.8064 | y: 165\n",
            "pred : 57.662933 | y: 261\n",
            "pred : 57.79779 | y: 24\n",
            "pred : 62.385937 | y: 28\n",
            "pred : 59.371517 | y: 23\n",
            "pred : 56.711594 | y: 23\n",
            "pred : 56.711594 | y: 23\n",
            "pred : 57.29476 | y: 47\n",
            "pred : 54.56853 | y: 22\n",
            "pred : 56.81564 | y: 24\n",
            "pred : 57.412357 | y: 74\n",
            "pred : 55.30172 | y: 21\n",
            "pred : 60.706192 | y: 120\n",
            "pred : 62.591656 | y: 73\n",
            "pred : 63.377106 | y: 40\n",
            "pred : 68.99247 | y: 54\n",
            "pred : 64.1539 | y: 24\n",
            "pred : 64.1539 | y: 24\n",
            "pred : 69.89566 | y: 82\n",
            "pred : 60.090252 | y: 147\n",
            "pred : 60.512703 | y: 273\n",
            "pred : 59.544975 | y: 504\n",
            "pred : 57.421093 | y: 10\n",
            "pred : 51.03863 | y: 25\n",
            "pred : 66.51356 | y: 71\n",
            "pred : 59.20249 | y: 25\n",
            "pred : 66.35852 | y: 47\n",
            "pred : 57.720345 | y: 51\n",
            "pred : 57.720345 | y: 51\n",
            "pred : 59.132267 | y: 20\n",
            "pred : 62.0251 | y: 146\n",
            "pred : 58.717716 | y: 93\n",
            "pred : 63.497528 | y: 6\n",
            "pred : 50.30809 | y: 41\n",
            "pred : 62.07025 | y: 135\n",
            "pred : 71.30224 | y: 33\n",
            "pred : 82.99468 | y: 300\n",
            "pred : 37.72212 | y: 82\n",
            "pred : 46.095715 | y: 672\n",
            "pred : 46.095715 | y: 672\n",
            "pred : 50.2111 | y: 111\n",
            "pred : 53.271774 | y: 56\n",
            "pred : 56.45721 | y: 48\n",
            "pred : 62.812336 | y: 39\n",
            "pred : 65.35699 | y: 24\n",
            "pred : 63.374233 | y: 31\n",
            "pred : 62.6829 | y: 24\n",
            "pred : 63.89209 | y: 301\n",
            "pred : 57.723343 | y: 32\n",
            "pred : 63.964336 | y: 45\n",
            "pred : 63.964336 | y: 45\n",
            "pred : 63.001583 | y: 68\n",
            "pred : 60.09266 | y: 28\n",
            "pred : 63.178387 | y: 94\n",
            "pred : 61.254566 | y: 46\n",
            "pred : 66.16579 | y: 78\n",
            "pred : 60.426113 | y: 25\n",
            "pred : 62.88561 | y: 63\n",
            "pred : 59.91069 | y: 56\n",
            "pred : 58.990723 | y: 48\n",
            "pred : 65.38127 | y: 63\n",
            "pred : 65.38127 | y: 63\n",
            "pred : 60.051723 | y: 71\n",
            "pred : 59.438934 | y: 121\n",
            "pred : 56.885414 | y: 119\n",
            "pred : 59.762123 | y: 95\n",
            "pred : 58.911198 | y: 120\n",
            "pred : 57.49863 | y: 24\n",
            "pred : 61.722424 | y: 119\n",
            "pred : 57.68023 | y: 47\n",
            "pred : 61.75366 | y: 504\n",
            "pred : 57.070656 | y: 96\n",
            "pred : 57.070656 | y: 96\n",
            "pred : 57.214386 | y: 24\n",
            "pred : 61.442696 | y: 23\n",
            "pred : 63.673504 | y: 195\n",
            "pred : 61.665558 | y: 94\n",
            "pred : 60.91015 | y: 71\n",
            "pred : 60.231018 | y: 168\n",
            "pred : 58.545273 | y: 95\n",
            "pred : 58.594345 | y: 48\n",
            "pred : 65.9542 | y: 335\n",
            "pred : 60.869644 | y: 23\n",
            "pred : 60.869644 | y: 23\n",
            "pred : 61.633488 | y: 167\n",
            "pred : 55.9305 | y: 79\n",
            "pred : 57.56011 | y: 45\n",
            "pred : 62.314644 | y: 43\n",
            "pred : 62.713993 | y: 74\n",
            "pred : 59.77472 | y: 96\n",
            "pred : 58.97894 | y: 76\n",
            "pred : 59.88864 | y: 69\n",
            "pred : 58.371166 | y: 21\n",
            "pred : 60.49995 | y: 53\n",
            "pred : 60.49995 | y: 53\n",
            "pred : 61.501934 | y: 40\n",
            "pred : 64.99474 | y: 24\n",
            "pred : 62.584106 | y: 27\n",
            "pred : 61.216316 | y: 19\n",
            "pred : 62.470776 | y: 49\n",
            "pred : 59.361923 | y: 6\n",
            "pred : 34.84424 | y: 15\n",
            "pred : 59.983925 | y: 6\n",
            "pred : 69.447266 | y: 16\n",
            "pred : 66.401886 | y: 9\n",
            "pred : 66.401886 | y: 9\n",
            "pred : 59.259686 | y: 92\n",
            "pred : 92.31935 | y: 41\n",
            "pred : 117.7969 | y: 107\n",
            "pred : 70.97862 | y: 61\n",
            "pred : 52.104477 | y: 334\n",
            "pred : 61.96432 | y: 107\n",
            "pred : 53.500134 | y: 60\n",
            "pred : 52.880062 | y: 168\n",
            "pred : 60.466984 | y: 442\n",
            "pred : 58.609768 | y: 20\n",
            "pred : 58.609768 | y: 20\n",
            "pred : 59.334385 | y: 23\n",
            "pred : 63.55381 | y: 191\n",
            "pred : 57.88931 | y: 219\n",
            "pred : 57.937855 | y: 86\n",
            "pred : 59.35159 | y: 20\n",
            "pred : 61.745827 | y: 455\n",
            "pred : 59.933323 | y: 542\n",
            "pred : 60.071915 | y: 16\n",
            "pred : 63.194443 | y: 168\n",
            "pred : 56.887775 | y: 503\n",
            "pred : 56.887775 | y: 503\n",
            "pred : 55.21424 | y: 27\n",
            "pred : 62.57411 | y: 123\n",
            "pred : 60.964455 | y: 314\n",
            "pred : 54.116825 | y: 689\n",
            "pred : 53.47237 | y: 45\n",
            "pred : 59.76429 | y: 56\n",
            "pred : 63.657356 | y: 68\n",
            "pred : 60.06948 | y: 47\n",
            "pred : 65.01874 | y: 99\n",
            "pred : 58.897465 | y: 65\n",
            "pred : 58.897465 | y: 65\n",
            "pred : 58.132484 | y: 103\n",
            "pred : 58.347626 | y: 20\n",
            "pred : 61.96137 | y: 47\n",
            "pred : 61.554832 | y: 95\n",
            "pred : 58.415886 | y: 67\n",
            "pred : 60.72976 | y: 103\n",
            "pred : 56.64735 | y: 61\n",
            "pred : 59.754715 | y: 58\n",
            "pred : 58.740597 | y: 112\n",
            "pred : 58.68228 | y: 223\n",
            "pred : 58.68228 | y: 223\n",
            "pred : 60.228115 | y: 125\n",
            "pred : 54.706406 | y: 26\n",
            "pred : 60.366558 | y: 22\n",
            "pred : 60.794834 | y: 63\n",
            "pred : 58.202435 | y: 144\n",
            "pred : 58.728443 | y: 119\n",
            "pred : 59.190834 | y: 119\n",
            "pred : 59.357506 | y: 505\n",
            "pred : 59.33535 | y: 143\n",
            "pred : 60.677628 | y: 23\n",
            "pred : 60.677628 | y: 23\n",
            "pred : 63.733467 | y: 47\n",
            "pred : 60.36719 | y: 96\n",
            "pred : 57.899742 | y: 19\n",
            "pred : 61.01017 | y: 97\n",
            "pred : 59.291794 | y: 48\n",
            "pred : 65.887215 | y: 101\n",
            "pred : 60.66672 | y: 90\n",
            "pred : 62.17381 | y: 55\n",
            "pred : 65.61933 | y: 21\n",
            "pred : 63.01306 | y: 40\n",
            "pred : 63.01306 | y: 40\n",
            "pred : 65.28672 | y: 180\n",
            "pred : 60.98257 | y: 408\n",
            "pred : 59.94564 | y: 131\n",
            "pred : 56.626583 | y: 831\n",
            "pred : 56.904106 | y: 124\n",
            "pred : 59.265137 | y: 239\n",
            "pred : 59.059734 | y: 119\n",
            "pred : 60.13064 | y: 112\n",
            "pred : 36.19068 | y: 295\n",
            "pred : 36.13799 | y: 42\n",
            "pred : 36.13799 | y: 42\n",
            "pred : 60.30289 | y: 220\n",
            "pred : 56.698425 | y: 280\n",
            "pred : 56.698425 | y: 130\n",
            "pred : 56.826122 | y: 94\n",
            "pred : 56.91282 | y: 191\n",
            "pred : 57.08282 | y: 64\n",
            "pred : 58.315166 | y: 48\n",
            "pred : 63.537144 | y: 199\n",
            "pred : 54.618877 | y: 48\n",
            "pred : 59.268528 | y: 65\n",
            "pred : 59.268528 | y: 65\n",
            "pred : 55.91946 | y: 192\n",
            "pred : 55.181263 | y: 161\n",
            "pred : 56.037975 | y: 123\n",
            "pred : 57.011906 | y: 74\n",
            "pred : 58.177044 | y: 429\n",
            "pred : 57.020226 | y: 69\n",
            "pred : 58.389374 | y: 59\n",
            "pred : 57.935066 | y: 108\n",
            "pred : 54.00863 | y: 76\n",
            "pred : 54.409416 | y: 90\n",
            "pred : 54.409416 | y: 90\n",
            "pred : 54.378525 | y: 177\n",
            "pred : 55.781055 | y: 89\n",
            "pred : 55.645622 | y: 45\n",
            "pred : 61.795517 | y: 52\n",
            "pred : 60.495018 | y: 78\n",
            "pred : 55.162453 | y: 18\n",
            "pred : 58.612873 | y: 22\n",
            "pred : 62.639397 | y: 78\n",
            "pred : 57.91667 | y: 39\n",
            "pred : 64.25037 | y: 25\n",
            "pred : 64.25037 | y: 25\n",
            "pred : 67.01922 | y: 115\n",
            "pred : 60.51983 | y: 96\n",
            "pred : 56.70516 | y: 49\n",
            "pred : 60.88204 | y: 94\n",
            "pred : 61.871292 | y: 167\n",
            "pred : 59.16953 | y: 24\n",
            "pred : 63.583 | y: 47\n",
            "pred : 62.761738 | y: 96\n",
            "pred : 58.969826 | y: 22\n",
            "pred : 61.753864 | y: 73\n",
            "pred : 61.753864 | y: 73\n",
            "pred : 59.62544 | y: 46\n",
            "pred : 64.16437 | y: 23\n",
            "pred : 66.79204 | y: 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-3780862dcf67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"pred : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" | y: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hrs_to_next\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1613\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m     \"\"\"\n\u001b[0;32m-> 1615\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   3956\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3957\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 3958\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   3959\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3960\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3136\u001b[0m               self._input_structure),\n\u001b[1;32m   3137\u001b[0m           \u001b[0mautograph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3138\u001b[0;31m           attributes=defun_kwargs)\n\u001b[0m\u001b[1;32m   3139\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3140\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m   3135\u001b[0m             \u001b[0mautograph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3136\u001b[0m             \u001b[0mautograph_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperimental_autograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3137\u001b[0;31m             experimental_relax_shapes=experimental_relax_shapes))\n\u001b[0m\u001b[1;32m   3138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m   \u001b[0;31m# This code path is for the `foo = tfe.defun(foo, ...)` use case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, python_function, name, input_signature, attributes, autograph, autograph_options, experimental_relax_shapes, capture_by_value)\u001b[0m\n\u001b[1;32m   2351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2353\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2354\u001b[0m     \u001b[0;31m# _descriptor_cache is a of instance of a class to an instance-specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m     \u001b[0;31m# `Function`, used to make sure defun-decorated methods create different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM18jYeylVl8",
        "colab_type": "text"
      },
      "source": [
        "Still need to:\n",
        "- Add callbacks (save the model after training) - SRAVYA \n",
        "- Extract a validation set from the current training set - SRAVYA\n",
        "- Extract a training set and testing set from the current set - ANDRES\n",
        "- Evaluate the model and experiment with adding back in other contextual variables\n",
        "- Modify the data and RNN to output a timestamp as well\n",
        "- Visualize our RNN's predictions"
      ]
    }
  ]
}